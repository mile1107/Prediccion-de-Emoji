{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport emoji\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ReduceLROnPlateau\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Goal of the project\n\nWe will be building an Emojifier by using word vector representations. Our model will take an input sentence and find the most appropriate emoji to be used with this sentence - from an assortment of 5 emoji's at its disposal.\n\n* Heart\n* Baseball\n* Smile\n* Disappointment\n* Fork and Knife\n","metadata":{"_uuid":"1f90f2bed5f1afdbc661a7090b4b1ed9c44d2294"}},{"cell_type":"markdown","source":"We need to use only the first two columns. First column contains the sentence and the second column contains the Emoji associated with the sentence.","metadata":{"_uuid":"4cfecd30315f0d8a30769b37d435ba016ae44c87"}},{"cell_type":"code","source":"train = pd.read_csv('../input/emojify/train_emoji.csv', header=None, usecols=[0,1])\ntest = pd.read_csv('../input/emojify/test_emoji.csv', header=None, usecols=[0,1])\ntrain.head()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split train and test into X and Y","metadata":{"_uuid":"cd2078b0833dcf08b68a1b54ce51330a5bef0ab0"}},{"cell_type":"code","source":"X_train, Y_train = train[0], train[1]\nX_test, Y_test = test[0], test[1]\nprint(f'Shape of X is: {X_train.shape}')\nprint(f'Shape of Y is: {Y_train.shape}')","metadata":{"_uuid":"8350b9247646d8c5ef40a319d100dad2bebaa024","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check a random sentence and emoji that has been assigned to it. \n\nAlso,lets find the maximum words any sentence in the set has as it will be required later.","metadata":{"_uuid":"b10d2c25e2463d232f3d1a9b00a33e6aaed786a6"}},{"cell_type":"code","source":"emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n                    \"1\": \":baseball:\",\n                    \"2\": \":smile:\",\n                    \"3\": \":disappointed:\",\n                    \"4\": \":fork_and_knife:\"}\n\ndef label_to_emoji(label):\n    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n\nprint(X_train[20], label_to_emoji(Y_train[20]))\n\nmaxWords = len(max(X_train, key=len).split())\nprint('Maximum words in sentence are:',maxWords)","metadata":{"_uuid":"4bc3a053c285271e9d5103f23beb67e5eacf69c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert Y's to one-hot vectors","metadata":{"_uuid":"a74d0c821ad473b43e3d215927dbc12ca9058252"}},{"cell_type":"code","source":"# Convert Y to one-hot vectors\nY_train_oh = pd.get_dummies(Y_train)\nprint(Y_train_oh.shape)","metadata":{"_uuid":"e091e9093af634ec983be973dec25bf209c93ea4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be using word vector representations of the words in the sentence so we need word vector representations of the words in the sentences. We will use the Glove vectors for this representation. Based on few iterations 100 d vectors seem to work best for this case.","metadata":{"_uuid":"f6c263d1fcc7bc092ae8d2c1d1f0ac6d2b6246b2","trusted":true}},{"cell_type":"code","source":"def read_glove_vecs(glove_file):\n    with open(glove_file, 'r') as f:\n        words = set()         # ensures unique values\n        word_to_vec_map = {}  # this will be a dictionary mapping words to their vectors\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}   # dictionary mapping words to their index in the dictionary\n        index_to_words = {}   # dictionary mapping index to the word in the dictionary\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map\n\nword_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')","metadata":{"_uuid":"8400089faa77e00dd3a9bd9547ba32bca36f8f92","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will be using Keras for implementation of the LSTM. We thus need to create an 'embedding layer'.","metadata":{"_uuid":"0d113f2bda3125b5c60ac067278fefc077ec484a"}},{"cell_type":"code","source":"def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    vocab_len = len(word_to_index) + 1               # +1 for Keras  \n    emb_dim = word_to_vec_map[\"happy\"].shape[0]      # dimensionality of your GloVe word vectors\n    \n    emb_matrix = np.zeros((vocab_len, emb_dim))      # Initialization with zeros\n    \n    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n\n    # Define Keras embedding layer with the correct output/input sizes\n    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n    \n    # Build the embedding layer\n    embedding_layer.build((None,))\n    \n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","metadata":{"_uuid":"faa5f421c5496ec098adb7ca6a25c8b61e0cd0e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now need to convert all training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence.","metadata":{"_uuid":"ccef9a5d1d10def093cdedad52057289b19c4823","trusted":true}},{"cell_type":"code","source":"def sentences_to_indices(X, word_to_index, max_len):\n    m = X.shape[0]                               # number of training examples\n    X_indices = np.zeros((m, max_len))           # Initialize with zeros\n    for i in range(m):\n        sentence_words = (X[i].lower()).split()  # split each sentence into words\n        j = 0\n        for w in sentence_words:\n            X_indices[i, j] = word_to_index[w]   # lookup index of word from vocabulary\n            j = j + 1\n            \n    return X_indices\n\nX_train_indices = sentences_to_indices(X_train, word_to_index, maxWords)","metadata":{"_uuid":"eadec7c2496f77809e7dcb9a917d5f6aab23e860","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets build the Emojifier model now.","metadata":{"_uuid":"9393d5512a24c5bb88881e11f2795ae70d336256"}},{"cell_type":"code","source":"def Emojify(input_shape, word_to_vec_map, word_to_index):\n    sentence_indices = Input(shape=input_shape, dtype='int32')\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    embeddings = embedding_layer(sentence_indices)\n    \n    X = LSTM(128, return_sequences=True)(embeddings)\n    X = Dropout(0.5)(X)\n    X = LSTM(128, return_sequences=False)(X)\n    X = Dropout(0.5)(X)\n    X = Dense(5, activation='softmax')(X)\n    X = Activation('softmax')(X)    \n    \n    model = Model(inputs=sentence_indices, outputs=X)\n    \n    return model\n\nemojifier = Emojify((maxWords,), word_to_vec_map, word_to_index)\nemojifier.summary()","metadata":{"_uuid":"1dfefb65eb1976085236557f083f9a5657e06af7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets define the loss, optimizer and metric to use.","metadata":{"_uuid":"ce302ec42925f1fa8af10b1311463f55844cc303"}},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\nemojifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nemojifier.fit(X_train_indices, Y_train_oh, epochs = 100, batch_size = 16, shuffle=True, \n                               callbacks=[reduce_lr])\n","metadata":{"_uuid":"ab3d3c89657c825731f701b70e003d3924c0bca7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxWords)\nY_test_oh = pd.get_dummies(Y_test)\nloss, acc = emojifier.evaluate(X_test_indices, Y_test_oh)\nprint()\nprint(\"Test accuracy = \", acc)","metadata":{"_uuid":"7c212183cbd1fe3fc9dc36d586bd57fea0d12c80","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test_oh = pd.get_dummies(Y_test)\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxWords)\npred = emojifier.predict(X_test_indices)\nfor i in range(len(X_test)):\n    x = X_test_indices\n    num = np.argmax(pred[i])\n    if(num != Y_test[i]):\n        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())","metadata":{"_uuid":"fa254d98572cbd069a5f340fd91cc5c7ba68ef35","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"ca49274bdf58031fb3a817421bfd1bc559b18ef7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"9d3a62c9a6fa4bc39fcd43e62fb9a8eb056dcf2e","trusted":true},"execution_count":null,"outputs":[]}]}