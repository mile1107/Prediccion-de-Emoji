{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import emoji\n","import matplotlib.pyplot as plt\n","\n","from keras.models import Model\n","from keras.layers import Dense, Input, Dropout, LSTM, Activation\n","from keras.layers.embeddings import Embedding\n","from keras.callbacks import ReduceLROnPlateau\n","\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{"_uuid":"1f90f2bed5f1afdbc661a7090b4b1ed9c44d2294"},"source":["# Goal of the project\n","\n","We will be building an Emojifier by using word vector representations. Our model will take an input sentence and find the most appropriate emoji to be used with this sentence - from an assortment of 5 emoji's at its disposal.\n","\n","* Heart\n","* Baseball\n","* Smile\n","* Disappointment\n","* Fork and Knife\n"]},{"cell_type":"markdown","metadata":{"_uuid":"4cfecd30315f0d8a30769b37d435ba016ae44c87"},"source":["We need to use only the first two columns. First column contains the sentence and the second column contains the Emoji associated with the sentence."]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>never talk to me again</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I am proud of your achievements</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>It is the worst day in my life</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Miss you so much</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>food is life</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 0  1\n","0           never talk to me again  3\n","1  I am proud of your achievements  2\n","2   It is the worst day in my life  3\n","3                 Miss you so much  0\n","4                     food is life  4"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv('DATOS_PROYECTO_4/train_emoji.csv', header=None, usecols=[0,1])\n","test = pd.read_csv('DATOS_PROYECTO_4/test_emoji.csv', header=None, usecols=[0,1])\n","train.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"cd2078b0833dcf08b68a1b54ce51330a5bef0ab0"},"source":["Split train and test into X and Y"]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"8350b9247646d8c5ef40a319d100dad2bebaa024","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X is: (132,)\n","Shape of Y is: (132,)\n"]}],"source":["X_train, Y_train = train[0], train[1]\n","X_test, Y_test = test[0], test[1]\n","print(f'Shape of X is: {X_train.shape}')\n","print(f'Shape of Y is: {Y_train.shape}')"]},{"cell_type":"markdown","metadata":{"_uuid":"b10d2c25e2463d232f3d1a9b00a33e6aaed786a6"},"source":["Lets check a random sentence and emoji that has been assigned to it. \n","\n","Also,lets find the maximum words any sentence in the set has as it will be required later."]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"4bc3a053c285271e9d5103f23beb67e5eacf69c4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["So sad you are not coming ðŸ˜ž\n","Maximum words in sentence are: 10\n"]}],"source":["emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n","                    \"1\": \":baseball:\",\n","                    \"2\": \":smile:\",\n","                    \"3\": \":disappointed:\",\n","                    \"4\": \":fork_and_knife:\"}\n","\n","def label_to_emoji(label):\n","    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n","\n","print(X_train[20], label_to_emoji(Y_train[20]))\n","\n","maxWords = len(max(X_train, key=len).split())\n","print('Maximum words in sentence are:',maxWords)"]},{"cell_type":"markdown","metadata":{"_uuid":"a74d0c821ad473b43e3d215927dbc12ca9058252"},"source":["Convert Y's to one-hot vectors"]},{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"e091e9093af634ec983be973dec25bf209c93ea4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(132, 5)\n"]}],"source":["# Convert Y to one-hot vectors\n","Y_train_oh = pd.get_dummies(Y_train)\n","print(Y_train_oh.shape)"]},{"cell_type":"markdown","metadata":{"_uuid":"f6c263d1fcc7bc092ae8d2c1d1f0ac6d2b6246b2","trusted":true},"source":["We will be using word vector representations of the words in the sentence so we need word vector representations of the words in the sentences. We will use the Glove vectors for this representation. Based on few iterations 100 d vectors seem to work best for this case."]},{"cell_type":"code","execution_count":15,"metadata":{"_uuid":"8400089faa77e00dd3a9bd9547ba32bca36f8f92","trusted":true},"outputs":[],"source":["def read_glove_vecs(glove_file):\n","    with open(glove_file,encoding='utf-8') as f:\n","        words = set()         # ensures unique values\n","        word_to_vec_map = {}  # this will be a dictionary mapping words to their vectors\n","        for line in f:\n","            line = line.strip().split()\n","            curr_word = line[0]\n","            words.add(curr_word)\n","            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","        \n","        i = 1\n","        words_to_index = {}   # dictionary mapping words to their index in the dictionary\n","        index_to_words = {}   # dictionary mapping index to the word in the dictionary\n","        for w in sorted(words):\n","            words_to_index[w] = i\n","            index_to_words[i] = w\n","            i = i + 1\n","    return words_to_index, index_to_words, word_to_vec_map\n","\n","word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('Glove_Embeddings/glove.6B.100d.txt')"]},{"cell_type":"markdown","metadata":{"_uuid":"0d113f2bda3125b5c60ac067278fefc077ec484a"},"source":["We will be using Keras for implementation of the LSTM. We thus need to create an 'embedding layer'."]},{"cell_type":"code","execution_count":16,"metadata":{"_uuid":"faa5f421c5496ec098adb7ca6a25c8b61e0cd0e0","trusted":true},"outputs":[],"source":["def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n","    vocab_len = len(word_to_index) + 1               # +1 for Keras  \n","    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # dimensionality of your GloVe word vectors\n","    \n","    emb_matrix = np.zeros((vocab_len, emb_dim))      # Initialization with zeros\n","    \n","    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n","    for word, index in word_to_index.items():\n","        emb_matrix[index, :] = word_to_vec_map[word]\n","\n","    # Define Keras embedding layer with the correct output/input sizes\n","    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n","    \n","    # Build the embedding layer\n","    embedding_layer.build((None,))\n","    \n","    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n","    embedding_layer.set_weights([emb_matrix])\n","    \n","    return embedding_layer"]},{"cell_type":"markdown","metadata":{"_uuid":"ccef9a5d1d10def093cdedad52057289b19c4823","trusted":true},"source":["We now need to convert all training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence."]},{"cell_type":"code","execution_count":17,"metadata":{"_uuid":"eadec7c2496f77809e7dcb9a917d5f6aab23e860","trusted":true},"outputs":[],"source":["def sentences_to_indices(X, word_to_index, max_len):\n","    m = X.shape[0]                               # number of training examples\n","    X_indices = np.zeros((m, max_len))           # Initialize with zeros\n","    for i in range(m):\n","        sentence_words = (X[i].lower()).split()  # split each sentence into words\n","        j = 0\n","        for w in sentence_words:\n","            X_indices[i, j] = word_to_index[w]   # lookup index of word from vocabulary\n","            j = j + 1\n","            \n","    return X_indices\n","\n","X_train_indices = sentences_to_indices(X_train, word_to_index, maxWords)"]},{"cell_type":"markdown","metadata":{"_uuid":"9393d5512a24c5bb88881e11f2795ae70d336256"},"source":["Lets build the Emojifier model now."]},{"cell_type":"code","execution_count":18,"metadata":{"_uuid":"1dfefb65eb1976085236557f083f9a5657e06af7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 10)]              0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 10, 100)           40000100  \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 10, 128)           117248    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 10, 128)           0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 128)               131584    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 5)                 645       \n","_________________________________________________________________\n","activation (Activation)      (None, 5)                 0         \n","=================================================================\n","Total params: 40,249,577\n","Trainable params: 249,477\n","Non-trainable params: 40,000,100\n","_________________________________________________________________\n"]}],"source":["def Emojify(input_shape, word_to_vec_map, word_to_index):\n","    sentence_indices = Input(shape=input_shape, dtype='int32')\n","    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n","    embeddings = embedding_layer(sentence_indices)\n","    \n","    X = LSTM(128, return_sequences=True)(embeddings)\n","    X = Dropout(0.5)(X)\n","    X = LSTM(128, return_sequences=False)(X)\n","    X = Dropout(0.5)(X)\n","    X = Dense(5, activation='softmax')(X)\n","    X = Activation('softmax')(X)    \n","    \n","    model = Model(inputs=sentence_indices, outputs=X)\n","    \n","    return model\n","\n","emojifier = Emojify((maxWords,), word_to_vec_map, word_to_index)\n","emojifier.summary()"]},{"cell_type":"markdown","metadata":{"_uuid":"ce302ec42925f1fa8af10b1311463f55844cc303"},"source":["Lets define the loss, optimizer and metric to use."]},{"cell_type":"code","execution_count":19,"metadata":{"_uuid":"ab3d3c89657c825731f701b70e003d3924c0bca7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","9/9 [==============================] - 2s 13ms/step - loss: 1.5979 - accuracy: 0.2576\n","Epoch 2/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.5665 - accuracy: 0.3030\n","Epoch 3/100\n","9/9 [==============================] - 0s 10ms/step - loss: 1.5502 - accuracy: 0.2955\n","Epoch 4/100\n","9/9 [==============================] - 0s 10ms/step - loss: 1.5010 - accuracy: 0.4242\n","Epoch 5/100\n","9/9 [==============================] - 0s 10ms/step - loss: 1.4409 - accuracy: 0.5379\n","Epoch 6/100\n","9/9 [==============================] - 0s 10ms/step - loss: 1.3355 - accuracy: 0.6439\n","Epoch 7/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.2825 - accuracy: 0.6742\n","Epoch 8/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.1850 - accuracy: 0.7500\n","Epoch 9/100\n","9/9 [==============================] - 0s 10ms/step - loss: 1.1412 - accuracy: 0.8106\n","Epoch 10/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.0950 - accuracy: 0.8106\n","Epoch 11/100\n","9/9 [==============================] - 0s 10ms/step - loss: 1.0964 - accuracy: 0.8106\n","Epoch 12/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.1352 - accuracy: 0.7955\n","Epoch 13/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.1059 - accuracy: 0.8030\n","\n","Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 14/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.2146 - accuracy: 0.6894\n","Epoch 15/100\n","9/9 [==============================] - 0s 11ms/step - loss: 1.0476 - accuracy: 0.8712\n","Epoch 16/100\n","9/9 [==============================] - 0s 10ms/step - loss: 1.0127 - accuracy: 0.9167\n","Epoch 17/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9823 - accuracy: 0.9394\n","Epoch 18/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9886 - accuracy: 0.9318\n","Epoch 19/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9730 - accuracy: 0.9394\n","Epoch 20/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9832 - accuracy: 0.9242\n","Epoch 21/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9651 - accuracy: 0.9470\n","Epoch 22/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9566 - accuracy: 0.9545\n","Epoch 23/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9523 - accuracy: 0.9621\n","Epoch 24/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9561 - accuracy: 0.9621\n","Epoch 25/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9510 - accuracy: 0.9621\n","Epoch 26/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9544 - accuracy: 0.9545\n","Epoch 27/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9593 - accuracy: 0.9470\n","Epoch 28/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9624 - accuracy: 0.9394\n","\n","Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 29/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9450 - accuracy: 0.9621\n","Epoch 30/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9449 - accuracy: 0.9621\n","Epoch 31/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9444 - accuracy: 0.9621\n","Epoch 32/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9408 - accuracy: 0.9697\n","Epoch 33/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9450 - accuracy: 0.9621\n","Epoch 34/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9368 - accuracy: 0.9697\n","Epoch 35/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9361 - accuracy: 0.9697\n","Epoch 36/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9378 - accuracy: 0.9697\n","Epoch 37/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9374 - accuracy: 0.9697\n","Epoch 38/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9359 - accuracy: 0.9697\n","Epoch 39/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9392 - accuracy: 0.9621\n","Epoch 40/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9373 - accuracy: 0.9697\n","Epoch 41/100\n","9/9 [==============================] - ETA: 0s - loss: 0.9455 - accuracy: 0.95 - 0s 10ms/step - loss: 0.9347 - accuracy: 0.9697\n","Epoch 42/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9346 - accuracy: 0.9697\n","Epoch 43/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9347 - accuracy: 0.9697\n","Epoch 44/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9343 - accuracy: 0.9697\n","Epoch 45/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9341 - accuracy: 0.9697\n","Epoch 46/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9341 - accuracy: 0.9697\n","Epoch 47/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9345 - accuracy: 0.9697\n","Epoch 48/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9339 - accuracy: 0.9773\n","Epoch 49/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9338 - accuracy: 0.9773\n","Epoch 50/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9340 - accuracy: 0.9697\n","Epoch 51/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9327 - accuracy: 0.9773\n","Epoch 52/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9326 - accuracy: 0.9773\n","Epoch 53/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9324 - accuracy: 0.9773\n","Epoch 54/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9326 - accuracy: 0.9773\n","Epoch 55/100\n","9/9 [==============================] - 0s 12ms/step - loss: 0.9315 - accuracy: 0.9773\n","Epoch 56/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9321 - accuracy: 0.9773\n","Epoch 57/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9314 - accuracy: 0.9773\n","Epoch 58/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9297 - accuracy: 0.9773\n","Epoch 59/100\n","9/9 [==============================] - 0s 12ms/step - loss: 0.9299 - accuracy: 0.9773\n","Epoch 60/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9303 - accuracy: 0.9773\n","Epoch 61/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9289 - accuracy: 0.9773\n","Epoch 62/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9292 - accuracy: 0.9773\n","Epoch 63/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9289 - accuracy: 0.9773\n","Epoch 64/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9285 - accuracy: 0.9773\n","Epoch 65/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9289 - accuracy: 0.9773\n","Epoch 66/100\n","9/9 [==============================] - 0s 12ms/step - loss: 0.9288 - accuracy: 0.9773\n","Epoch 67/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9283 - accuracy: 0.9773\n","Epoch 68/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9281 - accuracy: 0.9773\n","Epoch 69/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9289 - accuracy: 0.9773\n","Epoch 70/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9282 - accuracy: 0.9773\n","Epoch 71/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9284 - accuracy: 0.9773\n","\n","Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 72/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9282 - accuracy: 0.9773\n","Epoch 73/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9282 - accuracy: 0.9773\n","Epoch 74/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9287 - accuracy: 0.9773\n","\n","Epoch 00074: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 75/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9280 - accuracy: 0.9773\n","Epoch 76/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9285 - accuracy: 0.9773\n","Epoch 77/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9285 - accuracy: 0.9773\n","\n","Epoch 00077: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 78/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9284 - accuracy: 0.9773\n","Epoch 79/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9284 - accuracy: 0.9773\n","Epoch 80/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9279 - accuracy: 0.9773\n","Epoch 81/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9285 - accuracy: 0.9773\n","Epoch 82/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9286 - accuracy: 0.9773\n","Epoch 83/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9282 - accuracy: 0.9773\n","\n","Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","Epoch 84/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9285 - accuracy: 0.9773\n","Epoch 85/100\n","9/9 [==============================] - 0s 8ms/step - loss: 0.9276 - accuracy: 0.9773\n","Epoch 86/100\n","9/9 [==============================] - 0s 9ms/step - loss: 0.9284 - accuracy: 0.9773\n","Epoch 87/100\n","9/9 [==============================] - 0s 9ms/step - loss: 0.9285 - accuracy: 0.9773\n","Epoch 88/100\n","9/9 [==============================] - 0s 9ms/step - loss: 0.9289 - accuracy: 0.9773\n","\n","Epoch 00088: ReduceLROnPlateau reducing learning rate to 1e-05.\n","Epoch 89/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9283 - accuracy: 0.9773\n","Epoch 90/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9278 - accuracy: 0.9773\n","Epoch 91/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9283 - accuracy: 0.9773\n","Epoch 92/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9278 - accuracy: 0.9773\n","Epoch 93/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9283 - accuracy: 0.9773\n","Epoch 94/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9276 - accuracy: 0.9773\n","Epoch 95/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9279 - accuracy: 0.9773\n","Epoch 96/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9285 - accuracy: 0.9773\n","Epoch 97/100\n","9/9 [==============================] - 0s 10ms/step - loss: 0.9285 - accuracy: 0.9773\n","Epoch 98/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9283 - accuracy: 0.9773\n","Epoch 99/100\n","9/9 [==============================] - 0s 11ms/step - loss: 0.9286 - accuracy: 0.9773\n","Epoch 100/100\n","9/9 [==============================] - 0s 12ms/step - loss: 0.9281 - accuracy: 0.9773\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1f9921ef520>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n","emojifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","emojifier.fit(X_train_indices, Y_train_oh, epochs = 100, batch_size = 16, shuffle=True, \n","                               callbacks=[reduce_lr])\n"]},{"cell_type":"code","execution_count":20,"metadata":{"_uuid":"7c212183cbd1fe3fc9dc36d586bd57fea0d12c80","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2/2 [==============================] - 1s 5ms/step - loss: 1.2392 - accuracy: 0.6607\n","\n","Test accuracy =  0.6607142686843872\n"]}],"source":["X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxWords)\n","Y_test_oh = pd.get_dummies(Y_test)\n","loss, acc = emojifier.evaluate(X_test_indices, Y_test_oh)\n","print()\n","print(\"Test accuracy = \", acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"fa254d98572cbd069a5f340fd91cc5c7ba68ef35","trusted":true},"outputs":[],"source":["Y_test_oh = pd.get_dummies(Y_test)\n","X_test_indices = sentences_to_indices(X_test, word_to_index, maxWords)\n","pred = emojifier.predict(X_test_indices)\n","for i in range(len(X_test)):\n","    x = X_test_indices\n","    num = np.argmax(pred[i])\n","    if(num != Y_test[i]):\n","        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ca49274bdf58031fb3a817421bfd1bc559b18ef7","trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9d3a62c9a6fa4bc39fcd43e62fb9a8eb056dcf2e","trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":4}
