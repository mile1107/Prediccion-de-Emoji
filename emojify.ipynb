{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify\n",
    "## Map emoji to a text according to the context\n",
    "\n",
    "## What is covered?\n",
    "1. Data Engineering\n",
    "2. Load Embedding Vectors\n",
    "3. Train the model\n",
    "4. Test & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from util import Utils\n",
    "import numpy as np\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Label</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French macaroon is so tasty</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work is horrible</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am upset</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>throw the ball</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good joke</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Doc  Label  c3    c4\n",
       "0  French macaroon is so tasty      4 NaN   NaN\n",
       "1             work is horrible      3 NaN   NaN\n",
       "2                   I am upset      3 NaN   [3]\n",
       "3               throw the ball      1 NaN   [2]\n",
       "4                    Good joke      2 NaN   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load train & test csv files\n",
    "cols = [\"Doc\", \"Label\", \"c3\", \"c4\"]\n",
    "df = pd.read_csv(\"emojify_data.csv\", header=None, names = cols)\n",
    "df2 = pd.read_csv(\"test_emoji.csv\", header=None, names = cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels to Emoji\n",
    "<p>The text is labeled with integers range from 0-4. Each integer corresponds to a specific emoji.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 â¤ï¸\n",
      "label 1 âš¾\n",
      "label 2 ğŸ˜„\n",
      "label 3 ğŸ˜\n",
      "label 4 ğŸ´\n"
     ]
    }
   ],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    \n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "#function to convert integer to printable emoji\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
    "\n",
    "#print labels and respective emoji\n",
    "for i in range(5):\n",
    "    print(\"label\", i,label_to_emoji(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['French', 'macaroon', 'is', 'so', 'tasty'] ğŸ´\n",
      "['he', 'did', 'not', 'answer'] ğŸ˜\n"
     ]
    }
   ],
   "source": [
    "docs = df[\"Doc\"]\n",
    "labels = df[\"Label\"]\n",
    "docs_test = df2[\"Doc\"]\n",
    "labels_test = df2[\"Label\"]\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "#create tokenized documents and assign labels\n",
    "for i,doc in enumerate(docs):\n",
    "    X.append(doc.split())\n",
    "    y.append(labels[i])\n",
    "    \n",
    "for i,doc in enumerate(docs_test):\n",
    "    X_test.append(doc.split())\n",
    "    y_test.append(labels_test[i])\n",
    "    \n",
    "#print first example \n",
    "print(X[0],label_to_emoji(y[0]))\n",
    "print(X_test[1],label_to_emoji(y_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load GloVe Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = Utils()\n",
    "emb_file = 'D:\\Resources\\Glove_Embeddings\\glove.6B.50d.txt'\n",
    "dimention = 50\n",
    "word_to_index, index_to_word, word_to_vec_map = util.read_emb_vec(file_name=emb_file, dimention = dimention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#disable warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert tokenize docs to the indices representation of glove embedding\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = len(X)                                  \n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    for i,x in enumerate(X):\n",
    "        j = 0\n",
    "        # Loop over the tokens\n",
    "        for w in x:\n",
    "            X_indices[i, j] = word_to_index[w.lower()]\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = [['French', 'macaroon', 'is', 'so', 'tasty'], ['work', 'is', 'horrible']]\n",
      "X1_indices = [[153730. 229211. 192973. 336115. 353731.]\n",
      " [389837. 192973. 181872.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "example = [X[0],X[1]]\n",
    "example_indices = sentences_to_indices([X[0],X[1]],word_to_index, max_len = 5)\n",
    "print(\"X1 =\", example)\n",
    "print(\"X1_indices =\", example_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an embedding layer with GloVe Data for the Keras Model\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "   \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "    print(emb_matrix.shape)\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim,trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras emojify LSTM Model\n",
    "def emojify_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype = 'int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(input=sentence_indices, output=X)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8160/2945945010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmaxLen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetMaxLen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Max length of doc is \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "def getMaxLen(X):\n",
    "    max = 0\n",
    "    for x in X:\n",
    "        if len(x) > max:\n",
    "            max = len(x)\n",
    "    return max\n",
    "\n",
    "maxLen = getMaxLen(X)\n",
    "print(\"Max length of doc is \", maxLen)\n",
    "\n",
    "model = emojify_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X, word_to_index, maxLen)\n",
    "Y_train_oh = util.convert_to_one_hot(np.array(y), C = 5)\n",
    "\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "Y_test_oh = util.convert_to_one_hot(np.array(y_test), C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8160/689989682.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_oh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m35\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_oh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 35, batch_size = 1, shuffle=True, validation_data=(X_test_indices, Y_test_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  I want to eat , Expected emoji:ğŸ´, predicted ğŸ´\n",
      "Text:  he did not answer , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  he got a raise , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  she got me a present , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  ha ha ha it was so funny , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  he is a good friend , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  I am upset , Expected emoji:â¤ï¸, predicted ğŸ˜\n",
      "Text:  We had such a lovely dinner tonight , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  where is the food , Expected emoji:ğŸ´, predicted ğŸ´\n",
      "Text:  Stop making this joke ha ha ha , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  where is the ball , Expected emoji:âš¾, predicted âš¾\n",
      "Text:  work is hard , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  This girl is messing with me , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  are you serious ha ha , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  Let us go play baseball , Expected emoji:âš¾, predicted âš¾\n",
      "Text:  This stupid grader is not working , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  work is horrible , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  Congratulation for having a baby , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  stop messing around , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  any suggestions for dinner , Expected emoji:ğŸ´, predicted ğŸ´\n",
      "Text:  I love taking breaks , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  you brighten my day , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  I boiled rice , Expected emoji:ğŸ´, predicted ğŸ´\n",
      "Text:  she is a bully , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  Why are you feeling bad , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  I am upset , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  I worked during my birthday , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  My grandmother is the love of my life , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  enjoy your break , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  valentine day is near , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  I miss you so much , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  throw the ball , Expected emoji:âš¾, predicted âš¾\n",
      "Text:  My life is so boring , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  she said yes , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  will you be my valentine , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  he can pitch really well , Expected emoji:âš¾, predicted âš¾\n",
      "Text:  dance with me , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  I am starving , Expected emoji:ğŸ´, predicted ğŸ´\n",
      "Text:  See you at the restaurant , Expected emoji:ğŸ´, predicted ğŸ´\n",
      "Text:  I like to laugh , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  I will go dance , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  I like your jacket , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  i miss her , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  what is your favorite baseball game , Expected emoji:âš¾, predicted âš¾\n",
      "Text:  Good job , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  I love to the stars and back , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  What you did was awesome , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  ha ha ha lol , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  I want to joke , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  go away , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  yesterday we lost again , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  family is all I have , Expected emoji:â¤ï¸, predicted â¤ï¸\n",
      "Text:  you are failing this exercise , Expected emoji:ğŸ˜, predicted ğŸ˜\n",
      "Text:  Good joke , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  You totally deserve this prize , Expected emoji:ğŸ˜„, predicted ğŸ˜„\n",
      "Text:  I did not have breakfast , Expected emoji:ğŸ˜, predicted ğŸ˜\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "\n",
    "Y_test = y_test\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    sentence = \" \"\n",
    "    for w in X_test[i]:\n",
    "        sentence += (w + \" \")\n",
    "    if True:\n",
    "        print('Text: ' + sentence + ', Expected emoji:' + label_to_emoji(Y_test[i]) + ', predicted ' + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
